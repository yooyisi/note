
notes from Taiwan University ML course, see: https://www.youtube.com/watch?v=z95ZYgPgXOY

# I, Policy gradient

s: state, a: action

Trajectory: $\tau = {s_1, a_1, s_2, a_2 ... s_t, a_t}$

Probability of this trajectory: $P_{\theta}(\tau) = p(s_1)p(a_1|s_1)p(s_2|a_1,s_1)$ ...

In which: 

- $\theta$ is the parameters of our policy

- $p(a_i|s_i)$ is what we can control. We will adjust the policy to maxmize **Reward** 

Reward: $R(\tau) = \sum r_t$  is a random variable, only its expectation could be calculated
$$
E(R_\theta) = \sum_{\tau}R(\tau)P_{\theta}(\tau)
$$
$$
\Delta E(R_\theta) = \sum_{\tau}R(\tau)\Delta P_{\theta}(\tau)
$$
$$
= \sum_{\tau}R(\tau) P_{\theta}(\tau) \Delta log P_{\theta}(\tau)
$$
sampling
$$
\approx \frac{1}{N}\sum_{n=1}^{N}R(\tau^n) \Delta log P_{\theta}(\tau^n)
$$
because other parts are not derivable
$$
= \frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{tn} R(\tau^n) \Delta log P_{\theta}(a_t^n|s_t^n)
$$


#### tip 1, add a baseline
$E(R_\theta) \approx \frac{1}{N}\sum_{n=1}^{N} \sum_{t=1}^{tn} (R(\tau^n)-b) \Delta log P_{\theta}(a_t^n|s_t^n)$

if all rewards are positive, without baseline, the probability of not sampled actions will drop

#### tip 2, assign suitable credits
replace $R(\tau^n)$ in $(R(\tau^n)-b)$ by a time-relavant reward, time discount

# PPO - Proximal Policy Optimization
### On vs Off Policy
- on policy: learning and apply are the same (learn cheese by playing)
- off policy: learning and apply are different (learn cheese by watching)

**Problem of on-policy:**

Use policy $\pi_\theta$ to collect data, update parameters. After updating, the distribution $p_{\theta}(\tau)$ changed, you have to do sampling again
- time consuming

Off-policy, use another $\pi_\theta'$ to interact with env, sampling a lot of data, $\theta$ use those data to learn and update, until converge or bottle-neck, then generate new data with $\theta'$ again.

$$
E_{(s_t,a_t)\sim \pi_\theta}[A^{\theta}(s_t,a_t)\Delta logP_{\theta}(a_t^n|s_t^n))]
$$
$$
= E_{(s_t,a_t)\sim \pi_\theta}[\frac{P_{\theta}(a_t^n,s_t^n)}{P_{\theta'}(a_t^n,s_t^n)} A^{\theta'}(s_t,a_t)\Delta logP_{\theta}(a_t^n|s_t^n))]
$$
$$
J^{\theta'}(\theta) = E_{(s_t,a_t)\sim \pi_\theta}[\frac{P_{\theta}(a_t^n|s_t^n)}{P_{\theta'}(a_t^n|s_t^n)}A^{\theta'}(s_t,a_t)]
$$

### importance sampling
> why off-policy is feasible

We need sampling from distribution p to calculate its Expectation. It can be archived via sampling from another distribution q, as long as p and q are 0 at the same time.

But var is not the same, to calculate the expectation, due to the potentially huge variance difference, a big amount of sampling data is needed.


### PPO/TRPO
using $\theta'$ to update $\theta$, and $P_{\theta'}$ and $P_{\theta}$ should not be too different

$$
J_{PPO}^{\theta'} = J_{PPO}^{\theta'} - \beta KL(\theta,\theta')
$$

TRPO use constraint, hard to apply, performance similiar

# Q-Learning
**value based** not learn policy, learn critic: $V^{\pi}(s)$ state-value function

## Estimate $V^{\pi}(s)$
#### 1. Monte Carlo (MC)
play until the end of Episode, get reward $G_{a} = \sum r_t$

#### 2. Temporal-difference (TD)
...$s_t$, $a_t$, $r_t$, $s_{t+1}$...
$$
V^{\pi}(s_t) = r_t + V^{\pi}(s_{t+1})
$$
learn it now, not wait until the end of episode

#### MC vs TD
MC: large variance
TD: small variance, but not accurate

#### 3. $Q^{\pi}(s,a)$
Given $Q^{\pi}(s,a)$, find a new $\pi'$ "better" than $\pi$

"Better": $V^{\pi'}(s) >= V^{\pi}(s)$ for all state $s$
$$
\pi'(s) = \argmax_a Q^{\pi}(s,a)
$$
\pi' depends on Q, no other parameters

## Implementation tips
#### Target Network
$$
Q^{\pi}(s_t,a_t) = r_t + Q^{'\pi}(s_{t+1},a_{t+1})
$$
Q' is fixed to generate q value, Q is to update. After N times updating, Q' could be replaced by Q.

#### Exploration
some data is hard to be sampled by Q-function
**Epsilon Greedy** at a small probability $\epsilon$ action is choosed randomly, $\epsilon$ is decaying

#### Replay buffer
Buffer contains many samples, each sample: $s_t, a_t, r_t, s_{t+1}$

use some batch generated by different policies to train, and update the buffer. (just remember there is no problem with this process)

Reduce interactive time, increase diversity

## Optimizations of DQN
#### Double DQN
Q value is usually over-estimated
DQN: 
